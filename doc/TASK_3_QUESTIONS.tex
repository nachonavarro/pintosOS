\documentclass{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{caption}
\usepackage{multirow}
\usepackage{subcaption}
\renewcommand{\_}{\char`_}
\renewcommand{\tt}{\lstinline}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1} % Line spacing

\lstset{language=C,
                basicstyle=\ttfamily,
                keywordstyle=\color{blue}\ttfamily,
                morekeywords={bool},
                stringstyle=\color{red}\ttfamily,
                commentstyle=\color{Plum}\ttfamily,
                morecomment=[l][\color{magenta}]{\#}
}



% Set up the header and footer
\pagestyle{fancy}
\lhead{Group 18} % Top left header
\chead{Task 3: User Memory} % Top center head
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{Task 3: Virtual Memory}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ Tuesday,\ March\ 22,\ 2016}\\
\vspace{0.1in}\large{\textbf{Pintos Group 18}}
\vspace{3in}
}

\author{Corentin Herbinet, Ignacio Navarro, William Springsteen}
\date{}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle
\newpage

\section{Page Table Management}

\subsection{Data Structures}

\subsubsection{Purpose of Supplemental Page Table Data Structures}

\subsection{Algorithms}

\subsubsection{Locating Frame for Given Page}

Each process' supplemental page table is a hash table mapping between page addresses and their corresponding \texttt{struct spt\_entry}. The \texttt{struct spt\_entry} contains the address of the frame that contains the data for that page. For a given page, \texttt{get\_spt\_entry()} is called with the address of the page as an argument, and a pointer to the appropriate process' supplemental page table as the other argument. \texttt{get\_spt\_entry()} will return the \texttt{struct spt\_entry} for the given page, or return \texttt{NULL} if there is no \texttt{struct spt\_entry} in the table for that page. \texttt{get\_spt\_entry()} will simply call \texttt{hash\_find()} to find the \texttt{hash\_elem*} for the \texttt{struct spt\_entry} in the supplemental page table. The second argument to \texttt{hash\_find} is a \texttt{struct spt\_entry} that is allocated as a local variable, with it's \texttt{vaddr} member, which is the address of the page that this \texttt{struct spt\_entry} corresponds to, set to the address of the given page. It is ok to allocate this \texttt{struct} as a local variable because it is a fairly small data structure. A lock is not required here because \texttt{hash\_find()} only examines the hash table, and doesn't modify it. If \texttt{hash\_find()} returned \texttt{NULL}, \texttt{get\_spt\_entry()} will return \texttt{NULL} since there is no entry in the supplemental page table for the given page. Otherwise, we simply return the \texttt{struct spt\_entry} for the \texttt{hash\_elem*} using the \texttt{hash\_entry} macro. Now, to access the address of the frame containing the data of the given page, we can just read the \texttt{frame\_addr} member of the returned \texttt{struct spt\_entry}, after checking that the return value of \texttt{get\_spt\_entry()} wasn't \texttt{NULL}.

\subsubsection{Coordinating Accessed and Dirty Bits}

\subsection{Synchronisation}

\subsubsection{Race When Getting a New Frame}

\subsection{Rationale}

\subsubsection{Virtual-to-Physical Mappings}

Since the 80x86 architecture doesn't provide any way to directly access physical memory, Pintos maps kernel virtual memory directly to physical memory, which means that frames can be accessed through kernel virtual memory. We did not change this mapping because it was working perfectly well as it was. To map between a frame (physical address/kernel virtual address) and the user page contained within that frame, a frame table is used. Each entry in this frame table, a \texttt{struct fte}, contains the frame address and user page address, as well as the \texttt{pid} of its owner and a \texttt{struct list\_elem} so that it can be added to the frame table, which is a list. However, although this can be used as this could be used as the primary mapping between physical and virtual memory, we chose to use the supplemental page table as the main mapping for this, which leaves the frame table's main purpose being to obtain an unused frame - either by \texttt{palloc\_get\_page()}, or by eviction when the frame table is full. It seems like a better idea to use the supplemental page table for this mapping because the supplemental page table already holds lots of information about each page, so it made sense just to add a \texttt{frame\_addr} member to each supplemental page table entry, as it can be accessed the same way as the other pieces of information about that page in the supplemental page table. We chose to implement the supplemental page table as a hash table because, given the address of the page, hash tables allow for very efficient insertion and deletion over a wide range of table sizes. A hash table also made sense because each page has lots of information relating to it, so we can just map the page virtual address to a \texttt{struct} containing all information, and then grab whatever information we need.

\section{Paging To and From Disk}

\subsection{Data Structures}

\subsubsection{Purpose of Frame Table and Swap Table Data Structures}

\begin{enumerate}

\item

\begin{lstlisting}
struct fte {
  /* frame and upage are such that install_page(upage, kpage, _) will be
     called after frame_alloc() is called (kpage is returned from frame_alloc()
     and upage is passed as an argument, but both are stored in the fte). */
  void *frame; /* The frame itself, as the frame is 'just a page'. */
  void *upage; /* Pointer to page that currently occupies this frame. */
  pid_t owner; /* pid of process that owns this frame. */
  struct list_elem fte_elem; /* To allow each frame to be added to 'static
                                  struct list frames' in 'frame.c'. */
  uint64_t clock_counter;
};
\end{lstlisting}

\item

\begin{lstlisting}
static struct list frame_table;
static struct lock frame_table_lock;
\end{lstlisting}

\end{enumerate}

\begin{enumerate}

\item

We made a \texttt{struct fte} to be a frame table entry in the frame table, which is a \texttt{struct list}.

\begin{itemize}
\item

\texttt{frame} is the kernel virtual address of the frame itself. This address is obtained from the user pool using \texttt{palloc\_get\_page()}.

\item

\texttt{upage} is a pointer to the page that currently occupies the frame that this \texttt{fte} is for. This is passed as an argument when trying to allocate a frame using \texttt{frame\_alloc()}.

\item

\texttt{pid\_t owner} is the process id of the process that owns this frame. This is used in \texttt{save\_frame()} during eviction so that we can access the supplemental page table of that process, and so that we can clear that process' page directory when its frame is evicted.

\item

The \texttt{list\_elem} member allows each \texttt{fte} to be added to the frame table (a \texttt{struct list}). 

\item

\texttt{clock\_counter} is used in the second chance eviction algorithm, so we can order the frame table entries based on the \texttt{clock\_counter}.

\end{itemize}

\item

\begin{itemize}
\item

\texttt{frame\_table} is a global \texttt{struct list} of \texttt{struct fte}'s. It's main function is to obtain an unused frame. It is global since each process needs to obtain a frame from the same physical memory.

\item

\texttt{frame\_table\_lock} is required when modifying \texttt{frame\_table} (insertion and deletion), since \texttt{frame\_table} can be accessed by multiple processes.

\end{itemize}

\end{enumerate}

\subsection{Algorithms}

\subsubsection{Choosing a Frame to Evict}

\subsubsection{Adjusting Data Structures when Losing a Frame}

\subsubsection{Heuristic for Stack Growth}

\subsection{Synchronisation}

\subsubsection{Basics of VM Synchronisation Design}

For the Virtual Memory task, our synchronisation design was to use a lock for each table that was made - the frame table, swap table, memory map table, and supplemental page table. Each of these locks is acquired before modifying the respective table, and is released after modification has completed. This means that the Virtual Memory synchronisation design is consistent across the whole task, since locks are the only synchronisation primitive used here, and makes it very easy to understand. This design prevents deadlock because no process will try to acquire more than one Virtual Memory lock at a time. This means that there will be no circular waits, where each process in a circular chain is waiting on a lock that the next process in the chain has. Since this is one of the four necessary conditions for deadlock, deadlock is prevented.

\subsubsection{Page Fault Causing Eviction}

\subsubsection{Handling Access to Paged-Out Pages During System Calls}

\subsection{Rationale}

\subsubsection{Number of Locks}

Our VM synchronisation design consists of a medium amount of locks - four. We chose this many locks because we felt it was more logical to have a lock for each data structure that was made for this task, to make our code consistent and easier to understand. Also, we felt that this number of locks was the smallest amount of locks we could use to get rid of race conditions, yet there are not so many locks that parallelism is too limited. We needed at least this many locks so that when modifying the four VM data structures, we could ensure that race conditions were prevented.

\section{Memory Mapped Files}

\subsection{Data Structures}

\subsubsection{Purpose of File Mapping Table Data Structures}
\begin{enumerate}

\item

\begin{lstlisting}
/* The memory map table will be a hash table mapping a mapid_t to a
   struct mmap_mapping. */
struct mmap_mapping {
  struct hash_elem hash_elem;
  mapid_t mapid; /* Uniquely identifies the mapping (within the process). The
                    hash table mmap_table will take mapid as a key, and the
                    struct mmap_mapping it is in will be the value. */
  int num_pages; /* Number of pages that this file will take up when mapped.
                    If end file is just 1 byte into a page, that whole page is
                    needed. */
  void *start_uaddr; /* Start address that file is mapped to. */
  void *end_uaddr; /* End address that file is mapped to. */
  struct file *file; /* File that is mapped. Not the same struct as in
                        another mmap_mapping for same file, as
                        file_reopen() is used. */
};
\end{lstlisting}

\item

\begin{lstlisting}
struct thread 
 {
    .
    .
    /* Memory mapping members. */
    struct hash mmap_table; /* Mapping between mapid_t and struct mmap_mapping. */
    struct lock mmap_table_lock; /* Acquired/released before/after calling
                                    hash_insert()/hash_delete() on this
                                    threads mmap_table. */
    mapid_t next_mapid; /* Next mmap mapping for this thread will take this as its
                           mapid. Incremented after a new mapping is added.
                           Initially set to 0. */
    .
    .
 };
\end{lstlisting}

\end{enumerate}

\begin{enumerate}

\item

\begin{itemize}
\item
We created a new \texttt{struct}, \texttt{struct mmap\_mapping}, which will be an entry in the memory map table. 
\item
The first member of this \texttt{struct} is a \texttt{hash\_elem} so that this \texttt{struct} can be inserted into the memory map table for a thread.
\item
\texttt{mapid\_t mapid} uniquely identifies the mapping within its process. It's also used to lookup the \texttt{struct mmap\_mapping} that contains it in the mmap hash table. 
\item
\texttt{int num\_pages} is simply the number of pages that the mapped file will take up when it is mapped. \texttt{num\_pages} is needed in \texttt{sys\_munmap()} so that we know how many pages to remove from the process' list of virtual pages, and so we can write the page back to the file. 
\item
\texttt{start\_uaddr} is needed so that we can actually find the start of the file after it has been mapped, such as in \texttt{sys\_munmap} so that we can find the pages to remove from the process' list of virtual pages and write the page back to file. \texttt{end\_uaddr} can be used to work out the length of the file, and is also used to determine how many bytes 'stick out' on the final page.
\item
\texttt{struct file *file} is the actual file that has been mapped to the memory location. It is used to write certain pages back to \texttt{file} in \texttt{sys\_munmap()}.

\end{itemize}

\item

We added 3 members to \texttt{struct thread} for memory mapping. 

\begin{itemize}
\item
The first member is \texttt{mmap\_table} - the actual hash table for the memory mappings. This hash table will map \texttt{mapid}'s to \texttt{struct mmap\_mapping}'s.
\item
\texttt{mmap\_table\_lock} is required so that we can lock before calling \texttt{hash\_insert()} and \texttt{hash\_delete()}.
\item
\texttt{next\_mapid} is the \texttt{mapid} that will be given to the next mapping. It is incremented after each successful mapping, to ensure the uniqueness of a mapping's \texttt{mapid}.
\end{itemize}

\end{enumerate}

\subsection{Algorithms}

\subsubsection{Integrating Memory Mapped Files into Virtual Memory Subsystem}

Memory mapped files integrate quite seamlessly into our virtual memory subsystem. On a call to \texttt{sys\_mmap()}, each page of the memory mapped file will be inserted into the supplementary page table using \texttt{spt\_insert\_file()}, just as if it was a normal file that wasn't memory mapped. This means that the memory mapped file is ready to be lazily loaded just like any other entries in the supplementary page table.

\subsubsection{Overlapping File Mapping}

In \texttt{sys\_mmap()}, the number of pages that the file will have to be mapped to is calculated by doing \texttt{int pages = size / PGSIZE}, where \texttt{size} is the size in bytes of the file. However, since this is integer division, if \texttt{size} is not a multiple of \texttt{PGSIZE}, the final page won't be counted in \texttt{pages}. To fix this, we check to see if \texttt{size} is a multiple of \texttt{PGSIZE} using \texttt{size \% PGSIZE}. If this equals 0, then we can increment \texttt{pages} so that it is now correct. We then use a for loop, \texttt{for (i = 0; i < pages; i++)}, to check that no page in the file mapping overlaps with any existing segment. In this for loop, we use \texttt{get\_spt\_entry()} to check if this thread's supplementary page table has an entry for this page, which will be \texttt{((uint8\_t *) addr) + (i * PGSIZE))}, where \texttt{addr} is the address of the first page to be mapped to (this is the second argument to the \texttt{mmap} system call. If this function doesn't return \texttt{NULL}, then there is an overlap, and we can return -1 from \texttt{sys\_mmap()}.

\subsection{Rationale}

\subsubsection{Sharing Code Between \texttt{mmap} and Data Demand-Paging from Executables}

When inserting into the supplemental page table, both memory mappings and files get inserted using \texttt{spt\_insert\_file()}, which means that they both ready to be lazily loaded, and any further code for the lazy loading will be the same for both memory maps and files. The only code that is different is that \texttt{mmap} comes from the \texttt{mmap} system call, where we also have to check to see if the \texttt{mmap} is valid, insert into the \texttt{mmap\_table}, and set a unique \texttt{mapid}. Also, memory mappings can be unmapped using the \texttt{munmap} system call, which causes the mappings to potentially be written back to their files, so this code isn't shared. The only extra bit of code that could have been shared is the code that actually loops through each page and calls \texttt{spt\_insert\_file()} on each page. This code isn't shared because, although most of it is very similar, in \texttt{sys\_mmap()} we also need to check to see if we are overlapping with any existing pages in the supplementary page table. It makes a lot of sense to share the rest of the code, as code duplication is usually s bad thing, and in this case the code is still very easy to understand.

\end{document}